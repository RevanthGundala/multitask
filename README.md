# Multi-Task Manipulation with Shared Representation Learning

Train a single policy to solve multiple manipulation tasks using a shared trunk with task-specific heads. Demonstrates that shared representations learn faster and generalize better than training separate policies.

## ğŸ¯ Key Features

- **Shared Trunk Architecture**: Common feature extraction across all tasks with task-specific policy/value heads
- **4 Manipulation Tasks**: Reach, Push, Pick-and-Place, Peg Insertion
- **MJX + PPO**: GPU-accelerated physics with vectorized PPO training (2000+ parallel envs)
- **Fair Baseline Comparison**: Same hyperparameters for single-task vs multi-task
- **Comprehensive Evaluation**: Learning curves, sample efficiency, transfer analysis

## ğŸ—ï¸ Architecture

```
                    Observation (+ task one-hot)
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Shared Trunk       â”‚
                â”‚  (256 â†’ 256 MLP)      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼                   â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Policy  â”‚         â”‚  Value  â”‚         â”‚  Task   â”‚
   â”‚  Head   â”‚         â”‚  Head   â”‚         â”‚ Embed   â”‚
   â”‚ (128)   â”‚         â”‚ (128)   â”‚         â”‚ (opt)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚
        â–¼                   â–¼
     Actions              Value
```

## ğŸš€ Quick Start

```bash
# Install dependencies
uv sync

# Run quick demo (2 tasks, 500K steps, ~5 min on GPU)
uv run python main.py demo

# Full training (4 tasks, 10M steps each mode)
uv run python main.py train --mode both

# Compare results
uv run python main.py compare

# Render policy videos
uv run python main.py render
```

## ğŸ“Š Expected Results

| Metric | Multi-Task | Baseline (avg) |
|--------|------------|----------------|
| Sample Efficiency | 1x | ~2-3x more samples |
| Final Performance | Comparable | Comparable |
| Training Time | 1x | 4x (separate policies) |
| Transfer | âœ“ Positive | N/A |

## ğŸ”§ Configuration

Edit `configs/default.py` to customize:

```python
config.env.tasks = ("reach", "push", "pick_place", "peg_insert")
config.training.num_timesteps = 10_000_000
config.training.num_envs = 2048
config.network.shared_layer_sizes = (256, 256)
config.network.policy_head_sizes = (128,)
```

## ğŸ“ Project Structure

```
multitask/
â”œâ”€â”€ main.py                 # CLI entry point
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.py          # Training configuration
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ tasks/              # Individual task environments
â”‚   â”‚   â”œâ”€â”€ reach.py        # Reach target position
â”‚   â”‚   â”œâ”€â”€ push.py         # Push object to goal
â”‚   â”‚   â”œâ”€â”€ pick_place.py   # Pick and place object
â”‚   â”‚   â””â”€â”€ peg_insert.py   # Precision peg insertion
â”‚   â””â”€â”€ multitask_wrapper.py # Multi-task environment
â”œâ”€â”€ networks/
â”‚   â””â”€â”€ multitask_ppo.py    # Shared trunk + task heads
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_multitask.py  # Multi-task PPO training
â”‚   â””â”€â”€ train_baseline.py   # Single-task baselines
â””â”€â”€ evaluation/
    â”œâ”€â”€ compare.py          # Comparison plots
    â””â”€â”€ render.py           # Video rendering
```

## ğŸ–¥ï¸ Hardware Requirements

- **GPU**: NVIDIA GPU with 8GB+ VRAM (tested on RTX 4090)
- **RAM**: 16GB+ recommended
- **Training Time**: 
  - Demo: ~5 minutes
  - Full (multi-task only): ~30-60 minutes
  - Full (both modes): ~2-3 hours

## ğŸ“ˆ Visualizations

### TensorBoard (Real-time Training)

Monitor training progress in real-time with TensorBoard:

```bash
# Start TensorBoard (in a separate terminal)
uv run tensorboard --logdir logs

# Or specify a specific run
uv run tensorboard --logdir logs/multitask_ppo_2026*
```

Then open http://localhost:6006 in your browser.

**Metrics logged:**
- `reward/mean`: Average episode reward
- `reward/task_*`: Per-task rewards (reach, push, etc.)
- `loss/policy`: Policy loss
- `loss/value`: Value function loss
- `loss/entropy`: Entropy bonus
- `perf/fps`: Training throughput (frames/sec)

### Comparison Plots (After Training)

After training, the `compare` command generates:

1. **Learning Curves** (`learning_curves.png`): Per-task reward over time
2. **Sample Efficiency** (`sample_efficiency.png`): Steps to reach threshold
3. **Transfer Analysis** (`transfer_analysis.png`): Multi-task performance breakdown

The `render` command generates:

1. **Per-task Videos**: Individual task rollouts
2. **Task Montage**: 2x2 grid showing all tasks simultaneously

## ğŸ”¬ Research Context

This project demonstrates:

1. **Positive Transfer**: Shared representations accelerate learning on related tasks
2. **Sample Efficiency**: Multi-task training requires fewer total samples than N separate policies
3. **Practical Robotics**: Foundation for general-purpose manipulation systems

## ğŸ“œ License

MIT
